{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a253b8e1",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8449087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e960e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    MODEL_ID,\n",
    "    OUTPUT_DIR,\n",
    "    TEST_BATCH_SIZE,\n",
    "    LORA_R,\n",
    "    LORA_ALPHA,\n",
    "    LORA_DROPOUT,\n",
    "    DEVICE,\n",
    "    MAX_ANSWER_TOKENS,\n",
    "    ORPO_LAMBDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41a7bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865859d065c14d039b7922d0e5faaf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "TOKENIZER = processor.tokenizer\n",
    "EOS_ID = TOKENIZER.eos_token_id\n",
    "\n",
    "base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0657451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_helper import collate_fn\n",
    "from functools import partial\n",
    "\n",
    "collate_fn = partial(\n",
    "    collate_fn,\n",
    "    processor=processor,\n",
    "    DEVICE=DEVICE,\n",
    "    MAX_ANSWER_TOKENS=MAX_ANSWER_TOKENS,\n",
    "    TOKENIZER=TOKENIZER,\n",
    "    EOS_ID=EOS_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbc5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk(f\"{OUTPUT_DIR}/test_dataset\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4e3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from orpo_helper import answer_logits, loss_orpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc12999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    \n",
    "    loss_orpo_arr = []\n",
    "    loss_sft_arr = []\n",
    "    loss_or_arr = []\n",
    "\n",
    "\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
    "            chosen_logits, rejected_logits = answer_logits(model, prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask)\n",
    "\n",
    "            loss_orpo_val, loss_sft_val, loss_or_val = loss_orpo(\n",
    "                chosen_logits, \n",
    "                rejected_logits, \n",
    "                chosen_ids, \n",
    "                rejected_ids, \n",
    "                chosen_mask, \n",
    "                rejected_mask, \n",
    "                ORPO_LAMBDA\n",
    "            )\n",
    "\n",
    "            loss_orpo_arr.append(loss_orpo_val.item())\n",
    "            loss_sft_arr.append(loss_sft_val.item())\n",
    "            loss_or_arr.append(loss_or_val.item())\n",
    "\n",
    "    if training:\n",
    "        model.train()\n",
    "\n",
    "    loss_orpo_mean = sum(loss_orpo_arr) / len(loss_orpo_arr)\n",
    "    loss_sft_mean = sum(loss_sft_arr) / len(loss_sft_arr)\n",
    "    loss_or_mean = sum(loss_or_arr) / len(loss_or_arr)\n",
    "\n",
    "    return loss_orpo_mean, loss_sft_mean, loss_or_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017381d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70eb3979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19679a89d0a74bd38486160551ba17f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_results[\"base_model\"] = evaluate(base_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7a7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = [\n",
    "    {\n",
    "        \"name\": \"last\",\n",
    "        \"path\": f\"{OUTPUT_DIR}/last\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"best_exp_1\",\n",
    "        \"path\": f\"{OUTPUT_DIR}/step_2600\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"best_exp_2\",\n",
    "        \"path\": f\"{OUTPUT_DIR}/step_1400\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f84ab13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea5f9f47864590b5b9f98c4280746e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59c0d5e04174039a68c628914410eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cee5f6d89784417a3b8587359a5704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for adapter in adapters:\n",
    "    model.load_adapter(adapter[\"path\"], adapter_name=adapter[\"name\"])\n",
    "    model.set_adapter(adapter[\"name\"])\n",
    "    evaluation_results[adapter[\"name\"]] = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac538a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model': (14.390681205035971, 7.301905350719425, 0.7088912854091727),\n",
       " 'last': (13.170132643884893, 7.22701214028777, 0.5943498131182554),\n",
       " 'best_exp_1': (13.185364208633093, 7.062106564748201, 0.6123169823516187),\n",
       " 'best_exp_2': (13.597347122302159, 7.4391580485611515, 0.615810476618705)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fae0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../logs//merged/processor_config.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_adapter(adapters[0][\"path\"], adapter_name=adapters[0][\"name\"])\n",
    "model.set_adapter(\"last\")\n",
    "merged = model.merge_and_unload()\n",
    "merged.save_pretrained(f\"{OUTPUT_DIR}/merged\")\n",
    "processor.save_pretrained(f\"{OUTPUT_DIR}/merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af604987",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "from config import OUTPUT_DIR\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7be63513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381996b97d774d5987c26651341e59a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3ca94fa5314610b8da3b4153f023a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config\n",
    "merged_model_path = f\"{OUTPUT_DIR}/merged\"\n",
    "quantized_model_path = f\"{OUTPUT_DIR}/quantized\"\n",
    "\n",
    "max_seq_length = 4096 \n",
    "num_calibration_samples = 256 \n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(merged_model_path, use_fast=True)\n",
    "\n",
    "def cal_gen(num_samples):\n",
    "    ds = test_dataset\n",
    "    for item in itertools.islice(ds, num_samples):\n",
    "        image, question = item[\"image\"], item[\"question\"]\n",
    "\n",
    "        conv = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    {\"type\": \"image\"},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        prompt  = processor.apply_chat_template(conv, add_generation_prompt=True)\n",
    "        inputs  = processor(images=[image], text=[prompt],\n",
    "                            padding=False, return_tensors=\"pt\")\n",
    "\n",
    "        yield {\n",
    "            \"input_ids\":      inputs.input_ids[0].tolist(),\n",
    "            \"attention_mask\": inputs.attention_mask[0].tolist(),\n",
    "            \"pixel_values\":   inputs.pixel_values[0, 0].numpy().astype(\"float32\"),\n",
    "            # NEW  → pass the height/width pair the model expects\n",
    "            \"image_sizes\":    list(inputs.image_sizes[0]),          # e.g. [336, 336]\n",
    "        }\n",
    "\n",
    "# include the new column in the schema  ────────────────────────────────────────\n",
    "features = datasets.Features({\n",
    "    \"input_ids\":      datasets.Sequence(datasets.Value(\"int32\")),\n",
    "    \"attention_mask\": datasets.Sequence(datasets.Value(\"int8\")),\n",
    "    \"pixel_values\":   datasets.Array3D(shape=(3, 336, 336), dtype=\"float32\"),\n",
    "    \"image_sizes\":    datasets.Sequence(datasets.Value(\"int32\"), length=2),\n",
    "})\n",
    "\n",
    "calibration_dataset = Dataset.from_generator(\n",
    "    cal_gen,\n",
    "    gen_kwargs=dict(num_samples=num_calibration_samples),\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Now load the BIG model & run quantisation\n",
    "# ---------------------------------------------\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    merged_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cf6d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6d3595fda84169bd61de8b5fa01f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calibration_dataset.save_to_disk(f\"{OUTPUT_DIR}/calibration_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "132a7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12ac0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17T23:09:31.237105+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-07-17T23:09:31.244635+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-07-17T23:09:31.245550+0000 | _infer_mappings_from_model | INFO - No SmoothQuantModifier.mappings provided, inferring from model...\n",
      "2025-07-17T23:09:31.246440+0000 | get_layer_mappings_from_architecture | INFO - Architecture LlavaNextForConditionalGeneration not found in mappings. Using default mappings: [LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')]\n",
      "2025-07-17T23:09:33.539588+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-07-17T23:09:33.541270+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `SmoothQuantModifier`\n",
      "2025-07-17T23:09:33.754455+0000 | __init__ | WARNING - The following modules are call graph ancestors of sequential targets,but also contain offloaded modules: {'LlavaNextModel'}.\n",
      "These modules will not be traced, and any sequential target children will be executed jointly, which may lead to OOM errors\n",
      "2025-07-17T23:09:33.931665+0000 | trace_subgraphs | WARNING - Expected 56 subgraphs, but only traced 1. This is likely due to having wrapped code which calls sequential targets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 256/256 [00:22<00:00, 11.44it/s]\n",
      "(1/1): Calibrating:   0%|          | 0/256 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Raised an exception during execution of the following code:\n```\n1 \n2 torch.fx._symbolic_trace.wrap(\"transformers_models_llava_next_modeling_llava_next_wrapped_8749100517629\")\n3 \n4 def forward(self, input_ids : torch.Tensor, pixel_values : torch.Tensor, image_sizes : torch.Tensor, attention_mask : torch.Tensor):\n5     model = self.model(input_ids, pixel_values = pixel_values, image_sizes = image_sizes, vision_feature_layer = -2, vision_feature_select_strategy = 'default', attention_mask = attention_mask, position_ids = None, past_key_values = None, inputs_embeds = None, use_cache = False, output_attentions = False, output_hidden_states = False, return_dict = True, cache_position = None);  input_ids = pixel_values = image_sizes = attention_mask = None\n6     getitem = model[0]\n7     getattr_1 = model.past_key_values\n8     getattr_2 = model.hidden_states\n9     getattr_3 = model.attentions\n10     getattr_4 = model.image_hidden_states;  model = None\n11     getitem_1 = getitem[(slice(None, None, None), slice(0, None, None), slice(None, None, None))];  getitem = None\n12     lm_head = self.lm_head(getitem_1);  getitem_1 = None\n13     wrapped_8749100517629 = transformers_models_llava_next_modeling_llava_next_wrapped_8749100517629(lm_head, None, None)\n14     getitem_2 = wrapped_8749100517629[0];  wrapped_8749100517629 = None\n15     return {'loss': getitem_2, 'logits': lm_head, 'past_key_values': getattr_1, 'hidden_states': getattr_2, 'attentions': getattr_3, 'image_hidden_states': getattr_4}\n16     \n```\nThis is likely due to a violation of shape assumptions made when tracing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/pipelines/sequential/helpers.py:72\u001b[0m, in \u001b[0;36mSubgraph.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, attention_mask)\u001b[0m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:480\u001b[0m, in \u001b[0;36mLlavaNextModel.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 480\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:417\u001b[0m, in \u001b[0;36mLlavaNextModel.get_image_features\u001b[0;34m(self, pixel_values, image_sizes, vision_feature_layer, vision_feature_select_strategy)\u001b[0m\n\u001b[1;32m    416\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector(selected_image_feature)\n\u001b[0;32m--> 417\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_num_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/functional.py:222\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Overwriting reason:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# call here.\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_size_or_sections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/_tensor.py:1053\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_with_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 1 (input tensor's size at dimension 0), but got split_sizes=[3]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Quantization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43moneshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantized_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The number of samples is now controlled by the generator and this arg\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_calibration_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_calibration_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/entrypoints/oneshot.py:311\u001b[0m, in \u001b[0;36moneshot\u001b[0;34m(model, distill_teacher, config_name, tokenizer, processor, cache_dir, use_auth_token, precision, tie_word_embeddings, trust_remote_code_model, save_compressed, oneshot_device, model_revision, recipe, recipe_args, clear_sparse_session, stage, dataset, dataset_config_name, dataset_path, num_calibration_samples, shuffle_calibration_samples, max_seq_length, pad_to_max_length, text_column, concatenate_data, streaming, overwrite_cache, preprocessing_num_workers, min_tokens_per_module, trust_remote_code_data, output_dir, log_dir, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m local_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m one_shot \u001b[38;5;241m=\u001b[39m Oneshot(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocal_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mone_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m one_shot\u001b[38;5;241m.\u001b[39mmodel\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/entrypoints/oneshot.py:149\u001b[0m, in \u001b[0;36mOneshot.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mPerforms one-shot calibration.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m calibration_dataloader \u001b[38;5;241m=\u001b[39m get_calibration_dataloader(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_recipe_modifiers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecipe_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecipe_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m post_process(\n\u001b[1;32m    154\u001b[0m     model_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_args,\n\u001b[1;32m    155\u001b[0m     recipe_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecipe_args,\n\u001b[1;32m    156\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[1;32m    157\u001b[0m )\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/entrypoints/oneshot.py:193\u001b[0m, in \u001b[0;36mOneshot.apply_recipe_modifiers\u001b[0;34m(self, calibration_dataloader, recipe_stage)\u001b[0m\n\u001b[1;32m    191\u001b[0m modifiers \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_modifiers()\n\u001b[1;32m    192\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m CalibrationPipeline\u001b[38;5;241m.\u001b[39mfrom_modifiers(modifiers, user\u001b[38;5;241m=\u001b[39muser_pipeline)\n\u001b[0;32m--> 193\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m session\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/pipelines/independent/pipeline.py:49\u001b[0m, in \u001b[0;36mIndependentPipeline.__call__\u001b[0;34m(model, dataloader, dataset_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m pipeline_name \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     47\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/pipelines/sequential/pipeline.py:85\u001b[0m, in \u001b[0;36mSequentialPipeline.__call__\u001b[0;34m(model, dataloader, dataset_args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader)), desc\u001b[38;5;241m=\u001b[39mcalib_desc):\n\u001b[1;32m     84\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39mfetch(batch_idx, subgraph\u001b[38;5;241m.\u001b[39minput_names)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43msubgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m LifecycleCallbacks\u001b[38;5;241m.\u001b[39msequential_epoch_end()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# this pass does not trigger modifier hooks\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# and is only used for capturing outputs of newly compressed modules\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/llmcompressor/pipelines/sequential/helpers.py:74\u001b[0m, in \u001b[0;36mSubgraph.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m forward_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaised an exception during execution of the following code:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00madd_line_numbers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code\u001b[38;5;241m.\u001b[39msrc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is likely due to a violation of shape assumptions made when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexception\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Raised an exception during execution of the following code:\n```\n1 \n2 torch.fx._symbolic_trace.wrap(\"transformers_models_llava_next_modeling_llava_next_wrapped_8749100517629\")\n3 \n4 def forward(self, input_ids : torch.Tensor, pixel_values : torch.Tensor, image_sizes : torch.Tensor, attention_mask : torch.Tensor):\n5     model = self.model(input_ids, pixel_values = pixel_values, image_sizes = image_sizes, vision_feature_layer = -2, vision_feature_select_strategy = 'default', attention_mask = attention_mask, position_ids = None, past_key_values = None, inputs_embeds = None, use_cache = False, output_attentions = False, output_hidden_states = False, return_dict = True, cache_position = None);  input_ids = pixel_values = image_sizes = attention_mask = None\n6     getitem = model[0]\n7     getattr_1 = model.past_key_values\n8     getattr_2 = model.hidden_states\n9     getattr_3 = model.attentions\n10     getattr_4 = model.image_hidden_states;  model = None\n11     getitem_1 = getitem[(slice(None, None, None), slice(0, None, None), slice(None, None, None))];  getitem = None\n12     lm_head = self.lm_head(getitem_1);  getitem_1 = None\n13     wrapped_8749100517629 = transformers_models_llava_next_modeling_llava_next_wrapped_8749100517629(lm_head, None, None)\n14     getitem_2 = wrapped_8749100517629[0];  wrapped_8749100517629 = None\n15     return {'loss': getitem_2, 'logits': lm_head, 'past_key_values': getattr_1, 'hidden_states': getattr_2, 'attentions': getattr_3, 'image_hidden_states': getattr_4}\n16     \n```\nThis is likely due to a violation of shape assumptions made when tracing"
     ]
    }
   ],
   "source": [
    "# Quantization\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_dataset,\n",
    "    recipe=recipe,\n",
    "    output_dir=quantized_model_path,\n",
    "    # The number of samples is now controlled by the generator and this arg\n",
    "    num_calibration_samples=num_calibration_samples,\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7b6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68dc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
