{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMh8o9CVaj9ICPJkX1C/lN8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bc34b4a59744eb0b83001fe470a6efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5e68b2c41654c0a88f0a33d25c566ca",
              "IPY_MODEL_4eb4ff04a9154017819a2d8d91aa7594",
              "IPY_MODEL_db1e32da1e3c41d08001ee2f84fbdd20"
            ],
            "layout": "IPY_MODEL_b357a26e2974408f95ceff070ecdbbcb"
          }
        },
        "a5e68b2c41654c0a88f0a33d25c566ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdffe38e72e14589b8e02c05a0f7d460",
            "placeholder": "​",
            "style": "IPY_MODEL_60dfd26adca048c6927cb1c317701ae9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4eb4ff04a9154017819a2d8d91aa7594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c8ea12e1214f9e91e4cefa9f779f39",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_632c2b5a95e744be9671406be88e68ae",
            "value": 4
          }
        },
        "db1e32da1e3c41d08001ee2f84fbdd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3166a5cd4d493298f52075284f2354",
            "placeholder": "​",
            "style": "IPY_MODEL_88ab1a9621b844f692943960d7fb1077",
            "value": " 4/4 [05:41&lt;00:00, 70.87s/it]"
          }
        },
        "b357a26e2974408f95ceff070ecdbbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdffe38e72e14589b8e02c05a0f7d460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60dfd26adca048c6927cb1c317701ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15c8ea12e1214f9e91e4cefa9f779f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "632c2b5a95e744be9671406be88e68ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b3166a5cd4d493298f52075284f2354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ab1a9621b844f692943960d7fb1077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9729758f4b9c4410b8bc95e62cad67cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f99b3f4e8e4757ad38c91d52e1611d",
              "IPY_MODEL_053400e801a4462795fa9e419ac5cb6e",
              "IPY_MODEL_4524c55cb0434ff4b57fa375b4714221"
            ],
            "layout": "IPY_MODEL_15d0bfbe6dca4ab7b4f9232fca710300"
          }
        },
        "67f99b3f4e8e4757ad38c91d52e1611d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd86c15f1efc4c4c8f3572db97272742",
            "placeholder": "​",
            "style": "IPY_MODEL_fa2f5d1ddd174838a8816ba1d0643fa4",
            "value": "Epoch 1/1:   0%"
          }
        },
        "053400e801a4462795fa9e419ac5cb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f07b87485f341879da1bbded82ca170",
            "max": 7482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45d21ebd5c6246b6ae556416b0f265f4",
            "value": 0
          }
        },
        "4524c55cb0434ff4b57fa375b4714221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7545b54978394866b1857651c60a697c",
            "placeholder": "​",
            "style": "IPY_MODEL_b14c52d996af42318011f89b94a4770e",
            "value": " 0/7482 [00:00&lt;?, ?it/s]"
          }
        },
        "15d0bfbe6dca4ab7b4f9232fca710300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd86c15f1efc4c4c8f3572db97272742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa2f5d1ddd174838a8816ba1d0643fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f07b87485f341879da1bbded82ca170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d21ebd5c6246b6ae556416b0f265f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7545b54978394866b1857651c60a697c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14c52d996af42318011f89b94a4770e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grach0v/ORPO_LLaVA/blob/main/code/ORPO_LLAVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kB3aMilJKjvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82525a4d-d2bc-4da8-e03a-a6e0b2c121f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.8.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers trl datasets huggingface_hub bitsandbytes wandb tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive (optional)"
      ],
      "metadata": {
        "id": "eigtRq43XTRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_CACHE = \"/content/drive/MyDrive/hf_cache\""
      ],
      "metadata": {
        "id": "IHANN2aSilAm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, runtime\n",
        "import os\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# One shared cache for everything:\n",
        "HF_CACHE = \"/content/drive/MyDrive/hf_cache\"\n",
        "!mkdir -p \"$HF_CACHE\"\n",
        "\n",
        "os.environ[\"HF_HOME\"] = HF_CACHE           # generic root\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n",
        "os.environ[\"HF_DATASETS_CACHE\"] = HF_CACHE"
      ],
      "metadata": {
        "id": "TqTcrSFMzrMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c81b6cc-8ebf-4729-d03a-7ad0838c91db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4rR7z4T_XPKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import contextlib\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn.functional import log_softmax, softplus\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    LlavaNextForConditionalGeneration,\n",
        "    LlavaNextProcessor,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEBdNJX6Mv2M",
        "outputId": "86e10b33-36b0-4ebd-efb7-2f5100651f07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "eay3pB2CVv9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID          = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
        "DATASET_NAME      = \"openbmb/RLAIF-V-Dataset\"\n",
        "WANDB_PROJECT     = \"llava-qlora-orpo\"\n",
        "OUTPUT_DIR        = \"/content/drive/MyDrive/llava_orpo_adapters\"\n",
        "\n",
        "!mkdir -p \"$OUTPUT_DIR\"\n",
        "USE_QLORA         = True\n",
        "\n",
        "TRAIN_BATCH_SIZE  = 2 if USE_QLORA else 1\n",
        "VAL_BATCH_SIZE    = 16\n",
        "TEST_BATCH_SIZE   = 16\n",
        "GRAD_ACC_STEPS    = 4          # effective batch = TRAIN_BATCH_SIZE × GRAD_ACC_STEPS\n",
        "EPOCHS            = 1\n",
        "LEARNING_RATE     = 2e-4\n",
        "WARMUP_RATIO      = 0.03\n",
        "\n",
        "VAL_RATIO         = 0.05       # 5 % validation\n",
        "TEST_RATIO        = 0.05       # 5 % test\n",
        "\n",
        "LORA_R            = 8  if USE_QLORA else 16\n",
        "LORA_ALPHA        = 16 if USE_QLORA else 32\n",
        "\n",
        "LORA_DROPOUT      = 0.05\n",
        "ORPO_BETA         = 0.1\n",
        "\n",
        "LOG_EVERY_STEPS   = 4\n",
        "VAL_EVERY_STEPS   = 200\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "# Maximum number of tokens for the answer\n",
        "MAX_ANSWER_TOKENS = 128 # You can adjust this value"
      ],
      "metadata": {
        "id": "w7jOqSV3VzUt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "7w1a392FM07E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = LlavaNextProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
        "TOKENIZER = processor.tokenizer\n",
        "EOS_ID = TOKENIZER.eos_token_id\n",
        "\n",
        "if USE_QLORA:\n",
        "    # 4‑bit base + gradient‑ckpt\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    base_model = prepare_model_for_kbit_training(\n",
        "        base_model, use_gradient_checkpointing=True\n",
        "    )\n",
        "    base_model.config.use_cache = False     # must be OFF with grad‑ckpt\n",
        "else:\n",
        "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    base_model.config.use_cache = True      # keep cache on\n"
      ],
      "metadata": {
        "id": "NMzHaojPxewv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0bc34b4a59744eb0b83001fe470a6efb",
            "a5e68b2c41654c0a88f0a33d25c566ca",
            "4eb4ff04a9154017819a2d8d91aa7594",
            "db1e32da1e3c41d08001ee2f84fbdd20",
            "b357a26e2974408f95ceff070ecdbbcb",
            "fdffe38e72e14589b8e02c05a0f7d460",
            "60dfd26adca048c6927cb1c317701ae9",
            "15c8ea12e1214f9e91e4cefa9f779f39",
            "632c2b5a95e744be9671406be88e68ae",
            "8b3166a5cd4d493298f52075284f2354",
            "88ab1a9621b844f692943960d7fb1077"
          ]
        },
        "outputId": "b6ef595e-9e61-44df-bb8f-0addffed4553"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bc34b4a59744eb0b83001fe470a6efb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u6cqLn7WsDu",
        "outputId": "215a9344-6ead-4c4b-9f21-155cce322fbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 22,151,168 || all params: 7,588,898,816 || trainable%: 0.2919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataloader"
      ],
      "metadata": {
        "id": "Owbs85F6M87e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@contextlib.contextmanager\n",
        "def temporary_padding_side(tokenizer, side: str):\n",
        "    \"\"\"Temporarily change padding side (left/right) inside a `with` block.\"\"\"\n",
        "    original = tokenizer.padding_side\n",
        "    tokenizer.padding_side = side\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        tokenizer.padding_side = original\n",
        "\n",
        "\n",
        "def build_prompt_inputs(images: List[Image.Image], questions: List[str]) -> Dict[str, Tensor]:\n",
        "    \"\"\"Tokenise the (question + image placeholder) prompt with left‑padding.\"\"\"\n",
        "    conversations = [\n",
        "        [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": q}, {\"type\": \"image\"}]}]\n",
        "        for q in questions\n",
        "    ]\n",
        "    prompts = [processor.apply_chat_template(c, add_generation_prompt=True) for c in conversations]\n",
        "    encoded = processor(images=images, text=prompts, padding=True, return_tensors=\"pt\")\n",
        "    return {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "\n",
        "def tokenize_answers(texts: List[str], max_length: int | None = None) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"Right‑pad assistant answers and append EOS.\"\"\"\n",
        "\n",
        "    # Reason for the right pad tokenization -\n",
        "    # later I will concatenate prompt tokens and potential answer tokens,\n",
        "    # to get logits in one go, without writing a loop.\n",
        "    # Having pad tokens in the middle seems very confusing\n",
        "    # and can be misleading and couse errors in the future.\n",
        "\n",
        "    with temporary_padding_side(TOKENIZER, \"right\"):\n",
        "        encoded = TOKENIZER(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "    ids, mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
        "    eos_column = torch.full((ids.size(0), 1), EOS_ID, dtype=ids.dtype)\n",
        "    ids = torch.cat([ids, eos_column], dim=1)\n",
        "    mask = torch.cat([mask, torch.ones_like(eos_column)], dim=1)\n",
        "\n",
        "    if max_length is not None:\n",
        "        # Trim if longer than max_length\n",
        "        ids = ids[:, :max_length]\n",
        "        mask = mask[:, :max_length]\n",
        "\n",
        "    return ids.to(DEVICE), mask.to(DEVICE)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    questions = [item[\"question\"] for item in batch]\n",
        "    chosen_texts = [item[\"chosen\"] for item in batch]\n",
        "    rejected_texts = [item[\"rejected\"] for item in batch]\n",
        "\n",
        "    prompt_inputs = build_prompt_inputs(images, questions)\n",
        "    chosen_ids, chosen_mask = tokenize_answers(chosen_texts, max_length=MAX_ANSWER_TOKENS)\n",
        "    rejected_ids, rejected_mask = tokenize_answers(rejected_texts, max_length=MAX_ANSWER_TOKENS)\n",
        "\n",
        "    return prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask"
      ],
      "metadata": {
        "id": "BfaV050EJPIa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = load_dataset(DATASET_NAME, split=\"train[:20%]\") # Load only 20% of the training data\n",
        "first_split = raw_dataset.train_test_split(test_size=VAL_RATIO + TEST_RATIO, seed=42)\n",
        "train_dataset = first_split[\"train\"]\n",
        "val_test_dataset = first_split[\"test\"]\n",
        "val_fraction_of_tmp = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "second_split = val_test_dataset.train_test_split(test_size=1 - val_fraction_of_tmp, seed=42)\n",
        "val_dataset = second_split[\"train\"]\n",
        "test_dataset = second_split[\"test\"]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "e-Ex_V7WaTKp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-calculating the 95th percentile of the 'chosen' column length\n",
        "chosen_lengths = [len(text) for text in raw_dataset[\"chosen\"]]\n",
        "chosen_lengths.sort()\n",
        "quantile_95_length = chosen_lengths[int(len(chosen_lengths) * 0.95)]\n",
        "\n",
        "# Setting the model's max_length for generation\n",
        "model.generation_config.max_length = quantile_95_length\n",
        "print(f\"Set model.generation_config.max_length to {model.generation_config.max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsxnZ7U2dQul",
        "outputId": "1decb0f0-78a4-4ffc-988c-574214ad570b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set model.generation_config.max_length to 932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get logits"
      ],
      "metadata": {
        "id": "7s-hUm50NN5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_cache(prompt_inputs: Dict[str, Tensor], adapters_active: bool):\n",
        "    \"\"\"Run the prompt once; return `(last_logit, past_kv)`.\"\"\"\n",
        "    ctx = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
        "    training_mode = model.training\n",
        "    with ctx:\n",
        "        model.eval()                       # temporarily enable KV cache\n",
        "        with torch.no_grad():\n",
        "            out = model(**prompt_inputs, use_cache=True, return_dict=True)\n",
        "        if training_mode:                  # restore\n",
        "            model.train()\n",
        "    return out.logits[:, -1:, :], out.past_key_values\n",
        "\n",
        "if USE_QLORA:\n",
        "    # Overwrite get_prompt_cache\n",
        "    def get_prompt_cache(\n",
        "            prompt_inputs: Dict[str, Tensor],\n",
        "            adapters_active: bool\n",
        "    ) -> Tuple[Tensor, Tuple]:\n",
        "        \"\"\"\n",
        "        Return (last_prompt_logit, past_key_values) even when the model was\n",
        "        launched with gradient-checkpointing (QLoRA).\n",
        "\n",
        "        Strategy\n",
        "        --------\n",
        "        • Disable LoRA adapters if `adapters_active` is False.\n",
        "        • Walk every sub-module and flip `.gradient_checkpointing=False`.\n",
        "        • Switch to `eval()` and `torch.no_grad()` for the single prompt pass.\n",
        "        • Run with `use_cache=True` so the KV-cache is generated.\n",
        "        • Restore *all* flags exactly as they were.\n",
        "        \"\"\"\n",
        "        training_mode = model.training                   # remember .train/.eval state\n",
        "\n",
        "        # ── snapshot every layer’s gradient-ckpt flag ─────────────────────\n",
        "        ckpt_layers = []\n",
        "        for mod in model.modules():\n",
        "            if hasattr(mod, \"gradient_checkpointing\"):\n",
        "                ckpt_layers.append((mod, mod.gradient_checkpointing))\n",
        "                mod.gradient_checkpointing = False       # turn it off temporarily\n",
        "\n",
        "        # LoRA on/off context\n",
        "        adapter_ctx = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
        "\n",
        "        with adapter_ctx, torch.no_grad():\n",
        "            model.eval()                                 # no grads, cache allowed\n",
        "            out = model(**prompt_inputs, use_cache=True, return_dict=True)\n",
        "\n",
        "        # ── restore layer flags ───────────────────────────────────────────\n",
        "        for mod, orig_val in ckpt_layers:\n",
        "            mod.gradient_checkpointing = orig_val\n",
        "        if training_mode:\n",
        "            model.train()\n",
        "\n",
        "        return out.logits[:, -1:, :], out.past_key_values\n",
        "\n",
        "def sequence_logprob_from_cache(prompt_last_logit: Tensor,\n",
        "                                past_key_values,\n",
        "                                continuation_ids: Tensor,\n",
        "                                continuation_mask: Tensor,\n",
        "                                adapters_active: bool) -> Tensor:\n",
        "    \"\"\"Log‑probability of a continuation (incl. EOS) using an existing cache.\"\"\"\n",
        "    ctx = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
        "    with ctx:\n",
        "        logits_rest = model(\n",
        "            input_ids=continuation_ids[:, :-1],\n",
        "            attention_mask=continuation_mask[:, :-1],\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=False,\n",
        "            pixel_values=None,\n",
        "            image_sizes=None,\n",
        "            return_dict=True,\n",
        "        ).logits  # (B, N‑1, V)\n",
        "    full_logits = torch.cat([prompt_last_logit, logits_rest], dim=1)  # (B, N, V)\n",
        "    log_probs = log_softmax(full_logits, dim=-1)\n",
        "    token_lp = log_probs.gather(2, continuation_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    return (token_lp * continuation_mask).sum(dim=-1)  # (B,)\n"
      ],
      "metadata": {
        "id": "XGS7bdWEaqlM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def orpo_loss(lp_theta_c, lp_theta_r, lp_ref_c, lp_ref_r, beta=ORPO_BETA):\n",
        "    return softplus(-beta * ((lp_theta_c - lp_theta_r) - (lp_ref_c - lp_ref_r))).mean()\n",
        "\n",
        "# ───────────────────────────────────────────────────────────\n",
        "# Optimiser, scheduler, wandb\n",
        "# ───────────────────────────────────────────────────────────\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.0)\n",
        "steps_per_epoch = len(train_loader) // GRAD_ACC_STEPS\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    int(steps_per_epoch * EPOCHS * WARMUP_RATIO),\n",
        "    steps_per_epoch * EPOCHS,\n",
        ")\n",
        "\n",
        "wandb.init(project=WANDB_PROJECT, config={k: v for k, v in globals().items() if k.isupper()})\n",
        "\n",
        "# ───────────────────────────────────────────────────────────\n",
        "# Training Loop (with cache reuse)\n",
        "# ───────────────────────────────────────────────────────────\n",
        "\n",
        "model.train()\n",
        "acc_steps = 0\n",
        "running_loss = 0.0  # To accumulate loss for average\n",
        "for epoch in range(EPOCHS):\n",
        "    for global_step, batch in tqdm(enumerate(train_loader, 1), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
        "\n",
        "        # θ policy\n",
        "        last_theta, pkv_theta = get_prompt_cache(prompt_inputs, adapters_active=True)\n",
        "        lp_theta_chosen   = sequence_logprob_from_cache(last_theta, pkv_theta, chosen_ids,   chosen_mask,   adapters_active=True)\n",
        "        lp_theta_rejected = sequence_logprob_from_cache(last_theta, pkv_theta, rejected_ids, rejected_mask, adapters_active=True)\n",
        "\n",
        "        # reference policy\n",
        "        last_ref, pkv_ref = get_prompt_cache(prompt_inputs, adapters_active=False)\n",
        "        with torch.no_grad():\n",
        "            lp_ref_chosen   = sequence_logprob_from_cache(last_ref, pkv_ref, chosen_ids,   chosen_mask,   adapters_active=False)\n",
        "            lp_ref_rejected = sequence_logprob_from_cache(last_ref, pkv_ref, rejected_ids, rejected_mask, adapters_active=False)\n",
        "\n",
        "        loss = orpo_loss(lp_theta_chosen, lp_theta_rejected, lp_ref_chosen, lp_ref_rejected) / GRAD_ACC_STEPS\n",
        "        loss.backward()\n",
        "        acc_steps += 1\n",
        "        running_loss += loss.item() * GRAD_ACC_STEPS # Accumulate original loss before division\n",
        "\n",
        "\n",
        "        if acc_steps == GRAD_ACC_STEPS:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step(); scheduler.step(); optimizer.zero_grad(); acc_steps = 0\n",
        "\n",
        "            if global_step % LOG_EVERY_STEPS == 0:\n",
        "                avg_loss = running_loss / LOG_EVERY_STEPS\n",
        "                wandb.log(\n",
        "                    {\"train/orpo_loss\": avg_loss,\n",
        "                     \"lr\": scheduler.get_last_lr()[0],\n",
        "                     \"train/lp_theta_chosen\": lp_theta_chosen.mean().item(),\n",
        "                     \"train/lp_theta_rejected\": lp_theta_rejected.mean().item(),\n",
        "                     \"train/lp_ref_chosen\": lp_ref_chosen.mean().item(),\n",
        "                     \"train/lp_ref_rejected\": lp_ref_rejected.mean().item()},\n",
        "                    step=global_step\n",
        "                )\n",
        "                running_loss = 0.0 # Reset running loss\n",
        "\n",
        "\n",
        "            if global_step % VAL_EVERY_STEPS == 0:\n",
        "                model.eval()\n",
        "                val_losses = []\n",
        "                val_lp_theta_chosen_list = []\n",
        "                val_lp_theta_rejected_list = []\n",
        "                val_lp_ref_chosen_list = []\n",
        "                val_lp_ref_rejected_list = []\n",
        "                with torch.no_grad():\n",
        "                    for vb in tqdm(val_loader, desc=\"Validation\"):\n",
        "                        p_in, c_id, c_m, r_id, r_m = vb\n",
        "\n",
        "                        lt_last, lt_pkv = get_prompt_cache(p_in, adapters_active=True)\n",
        "                        lr_last, lr_pkv = get_prompt_cache(p_in, adapters_active=False)\n",
        "\n",
        "                        lt_c = sequence_logprob_from_cache(lt_last, lt_pkv, c_id, c_m, adapters_active=True)\n",
        "                        lt_r = sequence_logprob_from_cache(lt_last, lt_pkv, r_id, r_m, adapters_active=True)\n",
        "                        lr_c = sequence_logprob_from_cache(lr_last, lr_pkv, c_id, c_m, adapters_active=False)\n",
        "                        lr_r = sequence_logprob_from_cache(lr_last, lr_pkv, r_id, r_m, adapters_active=False)\n",
        "\n",
        "                        val_losses.append(orpo_loss(lt_c, lt_r, lr_c, lr_r).item())\n",
        "                        val_lp_theta_chosen_list.append(lt_c.mean().item())\n",
        "                        val_lp_theta_rejected_list.append(lt_r.mean().item())\n",
        "                        val_lp_ref_chosen_list.append(lr_c.mean().item())\n",
        "                        val_lp_ref_rejected_list.append(lr_r.mean().item())\n",
        "\n",
        "                wandb.log({\n",
        "                    \"val/orpo_loss\": sum(val_losses) / len(val_losses),\n",
        "                    \"val/lp_theta_chosen\": sum(val_lp_theta_chosen_list) / len(val_lp_theta_chosen_list),\n",
        "                    \"val/lp_theta_rejected\": sum(val_lp_theta_rejected_list) / len(val_lp_theta_rejected_list),\n",
        "                    \"val/lp_ref_chosen\": sum(val_lp_ref_chosen_list) / len(val_lp_ref_chosen_list),\n",
        "                    \"val/lp_ref_rejected\": sum(val_lp_ref_rejected_list) / len(val_lp_ref_rejected_list)\n",
        "                    }, step=global_step)\n",
        "                model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9729758f4b9c4410b8bc95e62cad67cf",
            "67f99b3f4e8e4757ad38c91d52e1611d",
            "053400e801a4462795fa9e419ac5cb6e",
            "4524c55cb0434ff4b57fa375b4714221",
            "15d0bfbe6dca4ab7b4f9232fca710300",
            "bd86c15f1efc4c4c8f3572db97272742",
            "fa2f5d1ddd174838a8816ba1d0643fa4",
            "9f07b87485f341879da1bbded82ca170",
            "45d21ebd5c6246b6ae556416b0f265f4",
            "7545b54978394866b1857651c60a697c",
            "b14c52d996af42318011f89b94a4770e"
          ]
        },
        "id": "9SYjobt5a6J9",
        "outputId": "dbceb45d-bbdf-4848-81c0-fb9abf2e4cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrach0v\u001b[0m (\u001b[33mcowboy_bebop\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250715_101743-kix3kgk7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/kix3kgk7' target=\"_blank\">glad-universe-9</a></strong> to <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo' target=\"_blank\">https://wandb.ai/cowboy_bebop/llava-qlora-orpo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/kix3kgk7' target=\"_blank\">https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/kix3kgk7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/1:   0%|          | 0/7482 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9729758f4b9c4410b8bc95e62cad67cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n",
            "Caching is incompatible with gradient checkpointing in MistralDecoderLayer. Setting `past_key_value=None`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_losses = []\n",
        "with torch.no_grad():\n",
        "    for tb in test_loader:\n",
        "        p_in, c_id, c_m, r_id, r_m = tb\n",
        "        lt_last, lt_pkv = get_prompt_cache(p_in, adapters_active=True)\n",
        "        lr_last, lr_pkv = get_prompt_cache(p_in, adapters_active=False)\n",
        "\n",
        "        lt_c = sequence_logprob_from_cache(lt_last, lt_pkv, c_id, c_m, adapters_active=True)\n",
        "        lt_r = sequence_logprob_from_cache(lt_last, lt_pkv, r_id, r_m, adapters_active=True)\n",
        "        lr_c = sequence_logprob_from_cache(lr_last, lr_pkv, c_id, c_m, adapters_active=False)\n",
        "        lr_r = sequence_logprob_from_cache(lr_last, lr_pkv, r_id, r_m, adapters_active=False)\n",
        "\n",
        "        test_losses.append(orpo_loss(lt_c, lt_r, lr_c, lr_r).item())\n",
        "\n",
        "wandb.log({\"test/orpo_loss\": sum(test_losses) / len(test_losses)})\n",
        "\n",
        "# ───────── save adapters & finish ─────────\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "LfNREqx9s_IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BD2hTAP7NllA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a052f1e4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}