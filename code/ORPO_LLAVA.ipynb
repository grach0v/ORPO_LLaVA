{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers trl datasets huggingface_hub bitsandbytes wandb tqdm pillow torchvision peft ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eigtRq43XTRQ"
   },
   "source": [
    "## Mount Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHANN2aSilAm"
   },
   "outputs": [],
   "source": [
    "# HF_CACHE = \"/content/drive/MyDrive/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqTcrSFMzrMB",
    "outputId": "3c81b6cc-8ebf-4729-d03a-7ad0838c91db"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive, runtime\n",
    "# import os\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# # One shared cache for everything:\n",
    "# HF_CACHE = \"/content/drive/MyDrive/hf_cache\"\n",
    "# !mkdir -p \"$HF_CACHE\"\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = HF_CACHE           # generic root\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = HF_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rR7z4T_XPKT"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEBdNJX6Mv2M",
    "outputId": "86e10b33-36b0-4ebd-efb7-2f5100651f07"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import log_softmax, softplus\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eay3pB2CVv9m"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7jOqSV3VzUt"
   },
   "outputs": [],
   "source": [
    "MODEL_ID          = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "DATASET_NAME      = \"openbmb/RLAIF-V-Dataset\"\n",
    "WANDB_PROJECT     = \"llava-qlora-orpo\"\n",
    "OUTPUT_DIR        = \"../logs/\"\n",
    "\n",
    "!mkdir -p \"$OUTPUT_DIR\"\n",
    "# USE_QLORA         = False \n",
    "# QLORA had a lot of issues with the Mistral model\n",
    "# SO I dropped it for now\n",
    "\n",
    "TRAIN_BATCH_SIZE  = 1 \n",
    "VAL_BATCH_SIZE    = 6\n",
    "TEST_BATCH_SIZE   = 6\n",
    "GRAD_ACC_STEPS    = 4          # effective batch = TRAIN_BATCH_SIZE × GRAD_ACC_STEPS\n",
    "EPOCHS            = 1\n",
    "LEARNING_RATE     = 2e-4\n",
    "WARMUP_RATIO      = 0.03\n",
    "\n",
    "VAL_RATIO         = 0.05       # 5 % validation\n",
    "TEST_RATIO        = 0.05       # 5 % test\n",
    "\n",
    "LORA_R            = 8  \n",
    "LORA_ALPHA        = 16 \n",
    "\n",
    "LORA_DROPOUT      = 0.05\n",
    "ORPO_LAMBDA       = 5\n",
    "\n",
    "LOG_EVERY_STEPS   = 4\n",
    "VAL_EVERY_STEPS   = 200\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# Maximum number of tokens for the answer\n",
    "MAX_ANSWER_TOKENS = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w1a392FM07E"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0bc34b4a59744eb0b83001fe470a6efb",
      "a5e68b2c41654c0a88f0a33d25c566ca",
      "4eb4ff04a9154017819a2d8d91aa7594",
      "db1e32da1e3c41d08001ee2f84fbdd20",
      "b357a26e2974408f95ceff070ecdbbcb",
      "fdffe38e72e14589b8e02c05a0f7d460",
      "60dfd26adca048c6927cb1c317701ae9",
      "15c8ea12e1214f9e91e4cefa9f779f39",
      "632c2b5a95e744be9671406be88e68ae",
      "8b3166a5cd4d493298f52075284f2354",
      "88ab1a9621b844f692943960d7fb1077"
     ]
    },
    "id": "NMzHaojPxewv",
    "outputId": "b6ef595e-9e61-44df-bb8f-0addffed4553"
   },
   "outputs": [],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "TOKENIZER = processor.tokenizer\n",
    "EOS_ID = TOKENIZER.eos_token_id\n",
    "\n",
    "base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# if USE_QLORA:\n",
    "#     # 4‑bit base + gradient‑ckpt\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "\n",
    "#     base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "#         MODEL_ID,\n",
    "#         quantization_config=bnb_config,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         device_map=\"auto\",\n",
    "#     )\n",
    "\n",
    "#     base_model = prepare_model_for_kbit_training(\n",
    "#         base_model, use_gradient_checkpointing=True\n",
    "#     )\n",
    "#     base_model.config.use_cache = False     # must be OFF with grad‑ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3u6cqLn7WsDu",
    "outputId": "215a9344-6ead-4c4b-9f21-155cce322fbe"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owbs85F6M87e"
   },
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfaV050EJPIa"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def temporary_padding_side(tokenizer, side: str):\n",
    "    \"\"\"Temporarily change padding side (left/right) inside a `with` block.\"\"\"\n",
    "    original = tokenizer.padding_side\n",
    "    tokenizer.padding_side = side\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        tokenizer.padding_side = original\n",
    "\n",
    "\n",
    "def build_prompt_inputs(images: List[Image.Image], questions: List[str]) -> Dict[str, Tensor]:\n",
    "    \"\"\"Tokenise the (question + image placeholder) prompt with left‑padding.\"\"\"\n",
    "    conversations = [\n",
    "        [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": q}, {\"type\": \"image\"}]}]\n",
    "        for q in questions\n",
    "    ]\n",
    "    prompts = [processor.apply_chat_template(c, add_generation_prompt=True) for c in conversations]\n",
    "    encoded = processor(images=images, text=prompts, padding=True, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "\n",
    "def tokenize_answers(texts: List[str], max_length: int | None = None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Right‑pad assistant answers and append EOS.\"\"\"\n",
    "\n",
    "    # Reason for the right pad tokenization -\n",
    "    # later I will concatenate prompt tokens and potential answer tokens,\n",
    "    # to get logits in one go, without writing a loop.\n",
    "    # Having pad tokens in the middle seems very confusing\n",
    "    # and can be misleading and couse errors in the future.\n",
    "\n",
    "    with temporary_padding_side(TOKENIZER, \"right\"):\n",
    "        encoded = TOKENIZER(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    ids, mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "    eos_column = torch.full((ids.size(0), 1), EOS_ID, dtype=ids.dtype)\n",
    "    ids = torch.cat([ids, eos_column], dim=1)\n",
    "    mask = torch.cat([mask, torch.ones_like(eos_column)], dim=1)\n",
    "\n",
    "    if max_length is not None:\n",
    "        # Trim if longer than max_length\n",
    "        ids = ids[:, :max_length]\n",
    "        mask = mask[:, :max_length]\n",
    "\n",
    "    return ids.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    chosen_texts = [item[\"chosen\"] for item in batch]\n",
    "    rejected_texts = [item[\"rejected\"] for item in batch]\n",
    "\n",
    "    prompt_inputs = build_prompt_inputs(images, questions)\n",
    "    chosen_ids, chosen_mask = tokenize_answers(chosen_texts, max_length=MAX_ANSWER_TOKENS)\n",
    "    rejected_ids, rejected_mask = tokenize_answers(rejected_texts, max_length=MAX_ANSWER_TOKENS)\n",
    "\n",
    "    return prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Ex_V7WaTKp"
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(DATASET_NAME, split=\"train[:5%]\")\n",
    "first_split = raw_dataset.train_test_split(test_size=VAL_RATIO + TEST_RATIO, seed=42)\n",
    "train_dataset = first_split[\"train\"]\n",
    "val_test_dataset = first_split[\"test\"]\n",
    "val_fraction_of_tmp = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "second_split = val_test_dataset.train_test_split(test_size=1 - val_fraction_of_tmp, seed=42)\n",
    "val_dataset = second_split[\"train\"]\n",
    "test_dataset = second_split[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s-hUm50NN5B"
   },
   "source": [
    "# Get logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_cache(prompt_inputs):\n",
    "    training  = model.training\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(**prompt_inputs, return_dict=True)\n",
    "\n",
    "    if training:\n",
    "        model.train()\n",
    "\n",
    "    return out.logits[:, -1:, :], out.past_key_values\n",
    "\n",
    "def answer_logits(prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask):\n",
    "    last_logits, prompt_kv = get_prompt_cache(prompt_inputs)\n",
    "\n",
    "    # raw logits when we feed the full answers\n",
    "    raw_chosen = model(\n",
    "        input_ids=chosen_ids,\n",
    "        attention_mask=chosen_mask,\n",
    "        past_key_values=prompt_kv\n",
    "    ).logits          # (B,N,V)\n",
    "\n",
    "    raw_rejected = model(\n",
    "        input_ids=rejected_ids,\n",
    "        attention_mask=rejected_mask,\n",
    "        past_key_values=prompt_kv\n",
    "    ).logits          # (B,N,V)\n",
    "\n",
    "    # align: prepend last_prompt_logits and drop the last timestep\n",
    "    chosen_logits = torch.cat([last_logits, raw_chosen[:, :-1, :]],  dim=1)\n",
    "    rejected_logits = torch.cat([last_logits, raw_rejected[:, :-1, :]], dim=1)\n",
    "\n",
    "    return chosen_logits, rejected_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_logp(logits: torch.Tensor, ids: torch.Tensor) -> torch.Tensor:\n",
    "    logp = log_softmax(logits, dim=-1)\n",
    "    return logp.gather(2, ids.unsqueeze(-1)).squeeze(-1)    # (B,N)\n",
    "\n",
    "def log_prob(logs, mask):\n",
    "    return (logs * mask).sum(dim=-1) / mask.sum(dim=-1)\n",
    "\n",
    "def log_odds(log_prob):\n",
    "    return log_prob - torch.log1p(-torch.exp(log_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_orpo(chosen_logits, rejected_logits, chosen_ids, rejected_ids, chosen_mask, rejected_mask, lam):\n",
    "    chosen_logits = token_logp(chosen_logits, chosen_ids)   # (B,N)\n",
    "    rejected_logits = token_logp(rejected_logits, rejected_ids)  # (B,N)\n",
    "\n",
    "    chosen_logp = log_prob(chosen_logits, chosen_mask)  # (B,)\n",
    "    rejected_logp = log_prob(rejected_logits, rejected_mask)  # (B,)\n",
    "    \n",
    "    log_odds_chosen = log_odds(chosen_logp)  # (B,)\n",
    "    log_odds_rejected = log_odds(rejected_logp)  # (B,)\n",
    "\n",
    "    L_sft = -chosen_logp.mean()  # supervised fine-tuning loss\n",
    "    L_or = -torch.log(\n",
    "        torch.sigmoid(log_odds_chosen - log_odds_rejected)\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "    return L_sft + lam * L_or, L_sft, L_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9729758f4b9c4410b8bc95e62cad67cf",
      "67f99b3f4e8e4757ad38c91d52e1611d",
      "053400e801a4462795fa9e419ac5cb6e",
      "4524c55cb0434ff4b57fa375b4714221",
      "15d0bfbe6dca4ab7b4f9232fca710300",
      "bd86c15f1efc4c4c8f3572db97272742",
      "fa2f5d1ddd174838a8816ba1d0643fa4",
      "9f07b87485f341879da1bbded82ca170",
      "45d21ebd5c6246b6ae556416b0f265f4",
      "7545b54978394866b1857651c60a697c",
      "b14c52d996af42318011f89b94a4770e"
     ]
    },
    "id": "9SYjobt5a6J9",
    "outputId": "dbceb45d-bbdf-4848-81c0-fb9abf2e4cb9"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.0)\n",
    "steps_per_epoch = len(train_loader) // GRAD_ACC_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    int(steps_per_epoch * EPOCHS * WARMUP_RATIO),\n",
    "    steps_per_epoch * EPOCHS,\n",
    ")\n",
    "\n",
    "wandb.init(project=WANDB_PROJECT, config={k: v for k, v in globals().items() if k.isupper()})\n",
    "\n",
    "wandb.watch(model, log=\"gradients\", log_freq=LOG_EVERY_STEPS)\n",
    "\n",
    "best_val = float(\"inf\")   # lower is better for ORPO\n",
    "best_step = -1\n",
    "\n",
    "\n",
    "model.train()\n",
    "acc_steps = 0\n",
    "running_loss = 0.0  # To accumulate loss for average\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for global_step, batch in tqdm(enumerate(train_loader, 1), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
    "\n",
    "        chosen_logits, rejected_logits = answer_logits(prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask)\n",
    "\n",
    "        loss_orpo_val, loss_sft_val, loss_or_val = loss_orpo(\n",
    "            chosen_logits, \n",
    "            rejected_logits, \n",
    "            chosen_ids, \n",
    "            rejected_ids, \n",
    "            chosen_mask, \n",
    "            rejected_mask, \n",
    "            ORPO_LAMBDA\n",
    "        )\n",
    "        \n",
    "        loss = loss_orpo_val / GRAD_ACC_STEPS \n",
    "        loss.backward()\n",
    "\n",
    "        acc_steps += 1\n",
    "        running_loss += loss.item() * GRAD_ACC_STEPS # Accumulate original loss before division\n",
    "        \n",
    "        if acc_steps == GRAD_ACC_STEPS:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step() \n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad() \n",
    "            acc_steps = 0\n",
    "\n",
    "            if global_step % LOG_EVERY_STEPS == 0:\n",
    "                avg_loss = running_loss / LOG_EVERY_STEPS\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/orpo_loss\": avg_loss,\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                        \"train/loss_orpo\": loss_orpo_val.item(),\n",
    "                        \"train/loss_sft\": loss_sft_val.item(),\n",
    "                        \"train/loss_or\": loss_or_val.item(),\n",
    "                    },\n",
    "                    step=global_step\n",
    "                )\n",
    "                running_loss = 0.0 # Reset running loss\n",
    "\n",
    "\n",
    "            if global_step % VAL_EVERY_STEPS == 0 or global_step == len(train_loader) - 1:\n",
    "                model.eval()\n",
    "                val_orpo_list, val_sft_list, val_or_list = [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                        prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
    "\n",
    "                        # single prompt pass, same as training\n",
    "                        chosen_logits, rejected_logits = answer_logits(\n",
    "                            prompt_inputs,\n",
    "                            chosen_ids,   chosen_mask,\n",
    "                            rejected_ids, rejected_mask\n",
    "                        )\n",
    "\n",
    "                        loss_orpo_val, loss_sft_val, loss_or_val = loss_orpo(\n",
    "                            chosen_logits, rejected_logits,\n",
    "                            chosen_ids, rejected_ids,\n",
    "                            chosen_mask, rejected_mask,\n",
    "                            ORPO_LAMBDA\n",
    "                        )\n",
    "\n",
    "                        val_orpo_list.append(loss_orpo_val.item())\n",
    "                        val_sft_list.append(loss_sft_val.item())\n",
    "                        val_or_list.append(loss_or_val.item())\n",
    "\n",
    "                mean_val_orpo = sum(val_orpo_list) / len(val_orpo_list)\n",
    "\n",
    "                # checkpoint if best\n",
    "                if mean_val_orpo < best_val:\n",
    "                    best_val = mean_val_orpo\n",
    "                    best_step = global_step\n",
    "                    ckpt_dir = f\"{OUTPUT_DIR}/step_{best_step}\"\n",
    "                    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "                    model.save_pretrained(ckpt_dir)\n",
    "                    wandb.run.summary.update({\n",
    "                        \"best_val_loss\": best_val,\n",
    "                        \"best_step\": best_step\n",
    "                    })\n",
    "                    print(f\"★ New best val_loss {best_val:.4f} at step {best_step} — adapters saved to {ckpt_dir}\")\n",
    "\n",
    "                wandb.log({\n",
    "                    \"val/orpo_loss\": mean_val_orpo,\n",
    "                    \"val/loss_sft\":  sum(val_sft_list) / len(val_sft_list),\n",
    "                    \"val/loss_or\":   sum(val_or_list)  / len(val_or_list),\n",
    "                }, step=global_step)\n",
    "\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfNREqx9s_IH"
   },
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "if best_step != -1:\n",
    "    best_ckpt_dir = f\"{OUTPUT_DIR}/step_{best_step}\"\n",
    "    print(f\"Loading best model from step {best_step}: {best_ckpt_dir}\")\n",
    "    model.load_adapter(best_ckpt_dir, adapter_name=\"best\")\n",
    "    model.set_adapter(\"best\")\n",
    "else:\n",
    "    print(\"No best model found, using current model state\")\n",
    "\n",
    "model.eval()\n",
    "test_orpo_list, test_sft_list, test_or_list = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
    "\n",
    "        chosen_logits, rejected_logits = answer_logits(\n",
    "            prompt_inputs,\n",
    "            chosen_ids, chosen_mask,\n",
    "            rejected_ids, rejected_mask\n",
    "        )\n",
    "\n",
    "        loss_orpo_val, loss_sft_val, loss_or_val = loss_orpo(\n",
    "            chosen_logits, rejected_logits,\n",
    "            chosen_ids, rejected_ids,\n",
    "            chosen_mask, rejected_mask,\n",
    "            ORPO_LAMBDA\n",
    "        )\n",
    "\n",
    "        test_orpo_list.append(loss_orpo_val.item())\n",
    "        test_sft_list.append(loss_sft_val.item())\n",
    "        test_or_list.append(loss_or_val.item())\n",
    "\n",
    "mean_test_orpo = sum(test_orpo_list) / len(test_orpo_list)\n",
    "mean_test_sft = sum(test_sft_list) / len(test_sft_list)\n",
    "mean_test_or = sum(test_or_list) / len(test_or_list)\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"ORPO Loss: {mean_test_orpo:.4f}\")\n",
    "print(f\"SFT Loss: {mean_test_sft:.4f}\")\n",
    "print(f\"OR Loss: {mean_test_or:.4f}\")\n",
    "\n",
    "wandb.log({\n",
    "    \"test/orpo_loss\": mean_test_orpo,\n",
    "    \"test/loss_sft\": mean_test_sft,\n",
    "    \"test/loss_or\": mean_test_or,\n",
    "})\n",
    "\n",
    "# ───────── save adapters & finish ─────────\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD2hTAP7NllA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a052f1e4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMh8o9CVaj9ICPJkX1C/lN8",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "053400e801a4462795fa9e419ac5cb6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f07b87485f341879da1bbded82ca170",
      "max": 7482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45d21ebd5c6246b6ae556416b0f265f4",
      "value": 0
     }
    },
    "0bc34b4a59744eb0b83001fe470a6efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5e68b2c41654c0a88f0a33d25c566ca",
       "IPY_MODEL_4eb4ff04a9154017819a2d8d91aa7594",
       "IPY_MODEL_db1e32da1e3c41d08001ee2f84fbdd20"
      ],
      "layout": "IPY_MODEL_b357a26e2974408f95ceff070ecdbbcb"
     }
    },
    "15c8ea12e1214f9e91e4cefa9f779f39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15d0bfbe6dca4ab7b4f9232fca710300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4524c55cb0434ff4b57fa375b4714221": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7545b54978394866b1857651c60a697c",
      "placeholder": "​",
      "style": "IPY_MODEL_b14c52d996af42318011f89b94a4770e",
      "value": " 0/7482 [00:00&lt;?, ?it/s]"
     }
    },
    "45d21ebd5c6246b6ae556416b0f265f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4eb4ff04a9154017819a2d8d91aa7594": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c8ea12e1214f9e91e4cefa9f779f39",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_632c2b5a95e744be9671406be88e68ae",
      "value": 4
     }
    },
    "60dfd26adca048c6927cb1c317701ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "632c2b5a95e744be9671406be88e68ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "67f99b3f4e8e4757ad38c91d52e1611d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd86c15f1efc4c4c8f3572db97272742",
      "placeholder": "​",
      "style": "IPY_MODEL_fa2f5d1ddd174838a8816ba1d0643fa4",
      "value": "Epoch 1/1:   0%"
     }
    },
    "7545b54978394866b1857651c60a697c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88ab1a9621b844f692943960d7fb1077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b3166a5cd4d493298f52075284f2354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9729758f4b9c4410b8bc95e62cad67cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67f99b3f4e8e4757ad38c91d52e1611d",
       "IPY_MODEL_053400e801a4462795fa9e419ac5cb6e",
       "IPY_MODEL_4524c55cb0434ff4b57fa375b4714221"
      ],
      "layout": "IPY_MODEL_15d0bfbe6dca4ab7b4f9232fca710300"
     }
    },
    "9f07b87485f341879da1bbded82ca170": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5e68b2c41654c0a88f0a33d25c566ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdffe38e72e14589b8e02c05a0f7d460",
      "placeholder": "​",
      "style": "IPY_MODEL_60dfd26adca048c6927cb1c317701ae9",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b14c52d996af42318011f89b94a4770e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b357a26e2974408f95ceff070ecdbbcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd86c15f1efc4c4c8f3572db97272742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1e32da1e3c41d08001ee2f84fbdd20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b3166a5cd4d493298f52075284f2354",
      "placeholder": "​",
      "style": "IPY_MODEL_88ab1a9621b844f692943960d7fb1077",
      "value": " 4/4 [05:41&lt;00:00, 70.87s/it]"
     }
    },
    "fa2f5d1ddd174838a8816ba1d0643fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdffe38e72e14589b8e02c05a0f7d460": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
