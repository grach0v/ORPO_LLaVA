{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /venv/main/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: trl in /venv/main/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: datasets in /venv/main/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /venv/main/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.10/site-packages (0.46.1)\n",
      "Requirement already satisfied: wandb in /venv/main/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow in /venv/main/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: peft in /venv/main/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: ipywidgets in /venv/main/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /venv/main/lib/python3.10/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /venv/main/lib/python3.10/site-packages (from trl) (1.8.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /venv/main/lib/python3.10/site-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /venv/main/lib/python3.10/site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /venv/main/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /venv/main/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /venv/main/lib/python3.10/site-packages (from wandb) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /venv/main/lib/python3.10/site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /venv/main/lib/python3.10/site-packages (from wandb) (2.33.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /venv/main/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /venv/main/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /venv/main/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /venv/main/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /venv/main/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /venv/main/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: decorator in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /venv/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /venv/main/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /venv/main/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /venv/main/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /venv/main/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers trl datasets huggingface_hub bitsandbytes wandb tqdm pillow torchvision peft ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eigtRq43XTRQ"
   },
   "source": [
    "## Mount Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IHANN2aSilAm"
   },
   "outputs": [],
   "source": [
    "# HF_CACHE = \"/content/drive/MyDrive/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqTcrSFMzrMB",
    "outputId": "3c81b6cc-8ebf-4729-d03a-7ad0838c91db"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive, runtime\n",
    "# import os\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# # One shared cache for everything:\n",
    "# HF_CACHE = \"/content/drive/MyDrive/hf_cache\"\n",
    "# !mkdir -p \"$HF_CACHE\"\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = HF_CACHE           # generic root\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = HF_CACHE\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = HF_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rR7z4T_XPKT"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEBdNJX6Mv2M",
    "outputId": "86e10b33-36b0-4ebd-efb7-2f5100651f07"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import log_softmax, softplus\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eay3pB2CVv9m"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w7jOqSV3VzUt"
   },
   "outputs": [],
   "source": [
    "MODEL_ID          = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "DATASET_NAME      = \"openbmb/RLAIF-V-Dataset\"\n",
    "WANDB_PROJECT     = \"llava-qlora-orpo\"\n",
    "OUTPUT_DIR        = \"../logs/\"\n",
    "\n",
    "!mkdir -p \"$OUTPUT_DIR\"\n",
    "USE_QLORA         = True\n",
    "\n",
    "TRAIN_BATCH_SIZE  = 2 if USE_QLORA else 1\n",
    "VAL_BATCH_SIZE    = 8\n",
    "TEST_BATCH_SIZE   = 8\n",
    "GRAD_ACC_STEPS    = 4          # effective batch = TRAIN_BATCH_SIZE × GRAD_ACC_STEPS\n",
    "EPOCHS            = 1\n",
    "LEARNING_RATE     = 2e-4\n",
    "WARMUP_RATIO      = 0.03\n",
    "\n",
    "VAL_RATIO         = 0.05       # 5 % validation\n",
    "TEST_RATIO        = 0.05       # 5 % test\n",
    "\n",
    "LORA_R            = 8  if USE_QLORA else 16\n",
    "LORA_ALPHA        = 16 if USE_QLORA else 32\n",
    "\n",
    "LORA_DROPOUT      = 0.05\n",
    "ORPO_BETA         = 0.1\n",
    "\n",
    "LOG_EVERY_STEPS   = 4\n",
    "VAL_EVERY_STEPS   = 100\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# Maximum number of tokens for the answer\n",
    "MAX_ANSWER_TOKENS = 128 # You can adjust this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w1a392FM07E"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0bc34b4a59744eb0b83001fe470a6efb",
      "a5e68b2c41654c0a88f0a33d25c566ca",
      "4eb4ff04a9154017819a2d8d91aa7594",
      "db1e32da1e3c41d08001ee2f84fbdd20",
      "b357a26e2974408f95ceff070ecdbbcb",
      "fdffe38e72e14589b8e02c05a0f7d460",
      "60dfd26adca048c6927cb1c317701ae9",
      "15c8ea12e1214f9e91e4cefa9f779f39",
      "632c2b5a95e744be9671406be88e68ae",
      "8b3166a5cd4d493298f52075284f2354",
      "88ab1a9621b844f692943960d7fb1077"
     ]
    },
    "id": "NMzHaojPxewv",
    "outputId": "b6ef595e-9e61-44df-bb8f-0addffed4553"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39edc00fd26849bdaacb99567cc500a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "TOKENIZER = processor.tokenizer\n",
    "EOS_ID = TOKENIZER.eos_token_id\n",
    "\n",
    "if USE_QLORA:\n",
    "    # 4‑bit base + gradient‑ckpt\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    base_model = prepare_model_for_kbit_training(\n",
    "        base_model, use_gradient_checkpointing=True\n",
    "    )\n",
    "    base_model.config.use_cache = False     # must be OFF with grad‑ckpt\n",
    "else:\n",
    "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    base_model.config.use_cache = True      # keep cache on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3u6cqLn7WsDu",
    "outputId": "215a9344-6ead-4c4b-9f21-155cce322fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,151,168 || all params: 7,588,898,816 || trainable%: 0.2919\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owbs85F6M87e"
   },
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BfaV050EJPIa"
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def temporary_padding_side(tokenizer, side: str):\n",
    "    \"\"\"Temporarily change padding side (left/right) inside a `with` block.\"\"\"\n",
    "    original = tokenizer.padding_side\n",
    "    tokenizer.padding_side = side\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        tokenizer.padding_side = original\n",
    "\n",
    "\n",
    "def build_prompt_inputs(images: List[Image.Image], questions: List[str]) -> Dict[str, Tensor]:\n",
    "    \"\"\"Tokenise the (question + image placeholder) prompt with left‑padding.\"\"\"\n",
    "    conversations = [\n",
    "        [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": q}, {\"type\": \"image\"}]}]\n",
    "        for q in questions\n",
    "    ]\n",
    "    prompts = [processor.apply_chat_template(c, add_generation_prompt=True) for c in conversations]\n",
    "    encoded = processor(images=images, text=prompts, padding=True, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "\n",
    "def tokenize_answers(texts: List[str], max_length: int | None = None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Right‑pad assistant answers and append EOS.\"\"\"\n",
    "\n",
    "    # Reason for the right pad tokenization -\n",
    "    # later I will concatenate prompt tokens and potential answer tokens,\n",
    "    # to get logits in one go, without writing a loop.\n",
    "    # Having pad tokens in the middle seems very confusing\n",
    "    # and can be misleading and couse errors in the future.\n",
    "\n",
    "    with temporary_padding_side(TOKENIZER, \"right\"):\n",
    "        encoded = TOKENIZER(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    ids, mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "    eos_column = torch.full((ids.size(0), 1), EOS_ID, dtype=ids.dtype)\n",
    "    ids = torch.cat([ids, eos_column], dim=1)\n",
    "    mask = torch.cat([mask, torch.ones_like(eos_column)], dim=1)\n",
    "\n",
    "    if max_length is not None:\n",
    "        # Trim if longer than max_length\n",
    "        ids = ids[:, :max_length]\n",
    "        mask = mask[:, :max_length]\n",
    "\n",
    "    return ids.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    chosen_texts = [item[\"chosen\"] for item in batch]\n",
    "    rejected_texts = [item[\"rejected\"] for item in batch]\n",
    "\n",
    "    prompt_inputs = build_prompt_inputs(images, questions)\n",
    "    chosen_ids, chosen_mask = tokenize_answers(chosen_texts, max_length=MAX_ANSWER_TOKENS)\n",
    "    rejected_ids, rejected_mask = tokenize_answers(rejected_texts, max_length=MAX_ANSWER_TOKENS)\n",
    "\n",
    "    return prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e-Ex_V7WaTKp"
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(DATASET_NAME, split=\"train[:5%]\")\n",
    "first_split = raw_dataset.train_test_split(test_size=VAL_RATIO + TEST_RATIO, seed=42)\n",
    "train_dataset = first_split[\"train\"]\n",
    "val_test_dataset = first_split[\"test\"]\n",
    "val_fraction_of_tmp = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
    "second_split = val_test_dataset.train_test_split(test_size=1 - val_fraction_of_tmp, seed=42)\n",
    "val_dataset = second_split[\"train\"]\n",
    "test_dataset = second_split[\"test\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s-hUm50NN5B"
   },
   "source": [
    "# Get logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XGS7bdWEaqlM"
   },
   "outputs": [],
   "source": [
    "def get_prompt_cache(prompt_inputs: Dict[str, Tensor], adapters_active: bool):\n",
    "    \"\"\"Run the prompt once; return `(last_logit, past_kv)`.\"\"\"\n",
    "    ctx = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
    "    training_mode = model.training\n",
    "    with ctx:\n",
    "        model.eval()                       # temporarily enable KV cache\n",
    "        with torch.no_grad():\n",
    "            out = model(**prompt_inputs, use_cache=True, return_dict=True)\n",
    "        if training_mode:                  # restore\n",
    "            model.train()\n",
    "    return out.logits[:, -1:, :], out.past_key_values\n",
    "\n",
    "if USE_QLORA:\n",
    "    # Overwrite get_prompt_cache\n",
    "    # I hope I'll find a fix for QLoRA later\n",
    "    def get_prompt_cache(\n",
    "            prompt_inputs: Dict[str, Tensor],\n",
    "            adapters_active: bool\n",
    "    ) -> Tuple[Tensor, Tuple]:\n",
    "\n",
    "        return None, None\n",
    "\n",
    "def sequence_logprob_from_cache(prompt_last_logit: Tensor,\n",
    "                                past_key_values,\n",
    "                                continuation_ids: Tensor,\n",
    "                                continuation_mask: Tensor,\n",
    "                                adapters_active: bool) -> Tensor:\n",
    "    \"\"\"Log‑probability of a continuation (incl. EOS) using an existing cache.\"\"\"\n",
    "    ctx = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
    "    with ctx:\n",
    "        logits_rest = model(\n",
    "            input_ids=continuation_ids[:, :-1],\n",
    "            attention_mask=continuation_mask[:, :-1],\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=False,\n",
    "            pixel_values=None,\n",
    "            image_sizes=None,\n",
    "            return_dict=True,\n",
    "        ).logits  # (B, N‑1, V)\n",
    "    full_logits = torch.cat([prompt_last_logit, logits_rest], dim=1)  # (B, N, V)\n",
    "    log_probs = log_softmax(full_logits, dim=-1)\n",
    "    token_lp = log_probs.gather(2, continuation_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return (token_lp * continuation_mask).sum(dim=-1)  # (B,)\n",
    "\n",
    "\n",
    "def sequence_logprob(prompt_inputs: Dict[str, Tensor],\n",
    "                     continuation_ids: Tensor,\n",
    "                     continuation_mask: Tensor,\n",
    "                     adapters_active: bool) -> Tensor:\n",
    "    \"\"\"\n",
    "    • Use cache when we have one.\n",
    "    • Otherwise: run full forward.\n",
    "      ─ reference policy  -> no grads\n",
    "      ─ θ‑policy          -> keep grads\n",
    "    \"\"\"\n",
    "    last_logit, pkv = get_prompt_cache(prompt_inputs, adapters_active)\n",
    "\n",
    "    if last_logit is not None and pkv is not None:      # fast cache path\n",
    "        return sequence_logprob_from_cache(\n",
    "            last_logit, pkv, continuation_ids, continuation_mask,\n",
    "            adapters_active=adapters_active\n",
    "        )\n",
    "\n",
    "    # ---------- fallback: no cache ----------\n",
    "    ctx_adapter = model.disable_adapter() if not adapters_active else contextlib.nullcontext()\n",
    "    ctx_grad    = torch.no_grad() if not adapters_active else contextlib.nullcontext()\n",
    "\n",
    "    with ctx_adapter, ctx_grad:\n",
    "        full_ids  = torch.cat([prompt_inputs[\"input_ids\"],\n",
    "                               continuation_ids[:, :-1]], dim=1)\n",
    "        full_mask = torch.cat([prompt_inputs[\"attention_mask\"],\n",
    "                               continuation_mask[:, :-1]], dim=1)\n",
    "\n",
    "        logits = model(\n",
    "            input_ids      = full_ids,\n",
    "            attention_mask = full_mask,\n",
    "            pixel_values   = prompt_inputs.get(\"pixel_values\"),\n",
    "            image_sizes    = prompt_inputs.get(\"image_sizes\"),\n",
    "            use_cache      = False,\n",
    "            return_dict    = True,\n",
    "        ).logits[:, -continuation_ids.size(1):, :]      # (B, N, V)\n",
    "\n",
    "    logp   = log_softmax(logits, -1)\n",
    "    tok_lp = logp.gather(2, continuation_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return (tok_lp * continuation_mask).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9729758f4b9c4410b8bc95e62cad67cf",
      "67f99b3f4e8e4757ad38c91d52e1611d",
      "053400e801a4462795fa9e419ac5cb6e",
      "4524c55cb0434ff4b57fa375b4714221",
      "15d0bfbe6dca4ab7b4f9232fca710300",
      "bd86c15f1efc4c4c8f3572db97272742",
      "fa2f5d1ddd174838a8816ba1d0643fa4",
      "9f07b87485f341879da1bbded82ca170",
      "45d21ebd5c6246b6ae556416b0f265f4",
      "7545b54978394866b1857651c60a697c",
      "b14c52d996af42318011f89b94a4770e"
     ]
    },
    "id": "9SYjobt5a6J9",
    "outputId": "dbceb45d-bbdf-4848-81c0-fb9abf2e4cb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrach0v\u001b[0m (\u001b[33mcowboy_bebop\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/ORPO_LLaVA/code/wandb/run-20250715_135418-2209r089</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/2209r089' target=\"_blank\">zany-oath-18</a></strong> to <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo' target=\"_blank\">https://wandb.ai/cowboy_bebop/llava-qlora-orpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/2209r089' target=\"_blank\">https://wandb.ai/cowboy_bebop/llava-qlora-orpo/runs/2209r089</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953effc2f1264df393435dcf1516489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/1871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/venv/main/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def orpo_loss(lp_theta_c, lp_theta_r, lp_ref_c, lp_ref_r, beta=ORPO_BETA):\n",
    "    return softplus(-beta * ((lp_theta_c - lp_theta_r) - (lp_ref_c - lp_ref_r))).mean()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# Optimiser, scheduler, wandb\n",
    "# ───────────────────────────────────────────────────────────\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.95), weight_decay=0.0)\n",
    "steps_per_epoch = len(train_loader) // GRAD_ACC_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    int(steps_per_epoch * EPOCHS * WARMUP_RATIO),\n",
    "    steps_per_epoch * EPOCHS,\n",
    ")\n",
    "\n",
    "wandb.init(project=WANDB_PROJECT, config={k: v for k, v in globals().items() if k.isupper()})\n",
    "\n",
    "wandb.watch(model, log=\"gradients\", log_freq=LOG_EVERY_STEPS)\n",
    "\n",
    "best_val = float(\"inf\")   # lower is better for ORPO\n",
    "best_step = -1\n",
    "\n",
    "\n",
    "model.train()\n",
    "acc_steps = 0\n",
    "running_loss = 0.0  # To accumulate loss for average\n",
    "for epoch in range(EPOCHS):\n",
    "    for global_step, batch in tqdm(enumerate(train_loader, 1), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        prompt_inputs, chosen_ids, chosen_mask, rejected_ids, rejected_mask = batch\n",
    "\n",
    "        # print(\n",
    "        #     f\"\"\"\n",
    "        #     input_ids {prompt_inputs[\"input_ids\"].size(1)} chosen_ids {chosen_ids.size(1)}, rejected_ids {rejected_ids.size(1)}, total {prompt_inputs[\"input_ids\"].size(1) + max(chosen_ids.size(1), rejected_ids.size(1))}\n",
    "        #     \"\"\",\n",
    "        #     end=\"\"\n",
    "        # )\n",
    "\n",
    "        lp_theta_chosen   = sequence_logprob(prompt_inputs, chosen_ids, chosen_mask, adapters_active=True)\n",
    "        lp_theta_rejected = sequence_logprob(prompt_inputs, rejected_ids, rejected_mask, adapters_active=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lp_ref_chosen   = sequence_logprob(prompt_inputs, chosen_ids, chosen_mask, adapters_active=False)\n",
    "            lp_ref_rejected = sequence_logprob(prompt_inputs, rejected_ids, rejected_mask, adapters_active=False)\n",
    "\n",
    "\n",
    "        loss = orpo_loss(lp_theta_chosen, lp_theta_rejected, lp_ref_chosen, lp_ref_rejected) / GRAD_ACC_STEPS\n",
    "        loss.backward()\n",
    "        acc_steps += 1\n",
    "        running_loss += loss.item() * GRAD_ACC_STEPS # Accumulate original loss before division\n",
    "\n",
    "\n",
    "        if acc_steps == GRAD_ACC_STEPS:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step(); scheduler.step(); optimizer.zero_grad(); acc_steps = 0\n",
    "\n",
    "            if global_step % LOG_EVERY_STEPS == 0:\n",
    "                avg_loss = running_loss / LOG_EVERY_STEPS\n",
    "                wandb.log(\n",
    "                    {\"train/orpo_loss\": avg_loss,\n",
    "                     \"lr\": scheduler.get_last_lr()[0],\n",
    "                     \"train/lp_theta_chosen\": lp_theta_chosen.mean().item(),\n",
    "                     \"train/lp_theta_rejected\": lp_theta_rejected.mean().item(),\n",
    "                     \"train/lp_ref_chosen\": lp_ref_chosen.mean().item(),\n",
    "                     \"train/lp_ref_rejected\": lp_ref_rejected.mean().item()},\n",
    "                    step=global_step\n",
    "                )\n",
    "                running_loss = 0.0 # Reset running loss\n",
    "\n",
    "\n",
    "            if global_step % VAL_EVERY_STEPS == 0:\n",
    "                model.eval()\n",
    "                val_losses = []\n",
    "                val_lp_theta_chosen_list = []\n",
    "                val_lp_theta_rejected_list = []\n",
    "                val_lp_ref_chosen_list = []\n",
    "                val_lp_ref_rejected_list = []\n",
    "                with torch.no_grad():\n",
    "                    for vb in tqdm(val_loader, desc=\"Validation\"):\n",
    "                        p_in, c_id, c_m, r_id, r_m = vb\n",
    "\n",
    "                        lt_last, lt_pkv = get_prompt_cache(p_in, adapters_active=True)\n",
    "                        lr_last, lr_pkv = get_prompt_cache(p_in, adapters_active=False)\n",
    "\n",
    "                        lt_c = sequence_logprob(p_in, c_id, c_m, adapters_active=True)\n",
    "                        lt_r = sequence_logprob(p_in, r_id, r_m, adapters_active=True)\n",
    "                        lr_c = sequence_logprob(p_in, c_id, c_m, adapters_active=False)\n",
    "                        lr_r = sequence_logprob(p_in, r_id, r_m, adapters_active=False)\n",
    "\n",
    "                        val_losses.append(orpo_loss(lt_c, lt_r, lr_c, lr_r).item())\n",
    "                        val_lp_theta_chosen_list.append(lt_c.mean().item())\n",
    "                        val_lp_theta_rejected_list.append(lt_r.mean().item())\n",
    "                        val_lp_ref_chosen_list.append(lr_c.mean().item())\n",
    "                        val_lp_ref_rejected_list.append(lr_r.mean().item())\n",
    "\n",
    "                val_loss = sum(val_losses) / len(val_losses)\n",
    "                if val_loss < best_val:\n",
    "                    best_val = val_loss\n",
    "                    best_step = global_step\n",
    "                    ckpt_dir = f\"{OUTPUT_DIR}/step_{best_step}\"\n",
    "                    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "                    model.save_pretrained(ckpt_dir)\n",
    "                    wandb.run.summary[\"best_val_loss\"] = best_val\n",
    "                    wandb.run.summary[\"best_step\"] = best_step\n",
    "                    print(f\"★ New best val_loss {best_val:.4f} at step {best_step} — adapters saved to {ckpt_dir}\")\n",
    " \n",
    "\n",
    "                wandb.log({\n",
    "                    \"val/orpo_loss\": sum(val_losses) / len(val_losses),\n",
    "                    \"val/lp_theta_chosen\": sum(val_lp_theta_chosen_list) / len(val_lp_theta_chosen_list),\n",
    "                    \"val/lp_theta_rejected\": sum(val_lp_theta_rejected_list) / len(val_lp_theta_rejected_list),\n",
    "                    \"val/lp_ref_chosen\": sum(val_lp_ref_chosen_list) / len(val_lp_ref_chosen_list),\n",
    "                    \"val/lp_ref_rejected\": sum(val_lp_ref_rejected_list) / len(val_lp_ref_rejected_list)\n",
    "                    }, step=global_step)\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfNREqx9s_IH"
   },
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "if best_step != -1:\n",
    "    best_ckpt_dir = f\"{OUTPUT_DIR}/step_{best_step}\"\n",
    "    print(f\"Loading best model from step {best_step}: {best_ckpt_dir}\")\n",
    "    model.load_adapter(best_ckpt_dir, adapter_name=\"best\")\n",
    "    model.set_adapter(\"best\")\n",
    "else:\n",
    "    print(\"No best model found, using current model state\")\n",
    "\n",
    "model.eval()\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for tb in test_loader:\n",
    "        p_in, c_id, c_m, r_id, r_m = tb\n",
    "        lt_last, lt_pkv = get_prompt_cache(p_in, adapters_active=True)\n",
    "        lr_last, lr_pkv = get_prompt_cache(p_in, adapters_active=False)\n",
    "\n",
    "        lt_c = sequence_logprob_from_cache(lt_last, lt_pkv, c_id, c_m, adapters_active=True)\n",
    "        lt_r = sequence_logprob_from_cache(lt_last, lt_pkv, r_id, r_m, adapters_active=True)\n",
    "        lr_c = sequence_logprob_from_cache(lr_last, lr_pkv, c_id, c_m, adapters_active=False)\n",
    "        lr_r = sequence_logprob_from_cache(lr_last, lr_pkv, r_id, r_m, adapters_active=False)\n",
    "\n",
    "        test_losses.append(orpo_loss(lt_c, lt_r, lr_c, lr_r).item())\n",
    "\n",
    "wandb.log({\"test/orpo_loss\": sum(test_losses) / len(test_losses)})\n",
    "\n",
    "# ───────── save adapters & finish ─────────\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD2hTAP7NllA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a052f1e4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMh8o9CVaj9ICPJkX1C/lN8",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "053400e801a4462795fa9e419ac5cb6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f07b87485f341879da1bbded82ca170",
      "max": 7482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45d21ebd5c6246b6ae556416b0f265f4",
      "value": 0
     }
    },
    "0bc34b4a59744eb0b83001fe470a6efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5e68b2c41654c0a88f0a33d25c566ca",
       "IPY_MODEL_4eb4ff04a9154017819a2d8d91aa7594",
       "IPY_MODEL_db1e32da1e3c41d08001ee2f84fbdd20"
      ],
      "layout": "IPY_MODEL_b357a26e2974408f95ceff070ecdbbcb"
     }
    },
    "15c8ea12e1214f9e91e4cefa9f779f39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15d0bfbe6dca4ab7b4f9232fca710300": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4524c55cb0434ff4b57fa375b4714221": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7545b54978394866b1857651c60a697c",
      "placeholder": "​",
      "style": "IPY_MODEL_b14c52d996af42318011f89b94a4770e",
      "value": " 0/7482 [00:00&lt;?, ?it/s]"
     }
    },
    "45d21ebd5c6246b6ae556416b0f265f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4eb4ff04a9154017819a2d8d91aa7594": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c8ea12e1214f9e91e4cefa9f779f39",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_632c2b5a95e744be9671406be88e68ae",
      "value": 4
     }
    },
    "60dfd26adca048c6927cb1c317701ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "632c2b5a95e744be9671406be88e68ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "67f99b3f4e8e4757ad38c91d52e1611d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd86c15f1efc4c4c8f3572db97272742",
      "placeholder": "​",
      "style": "IPY_MODEL_fa2f5d1ddd174838a8816ba1d0643fa4",
      "value": "Epoch 1/1:   0%"
     }
    },
    "7545b54978394866b1857651c60a697c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88ab1a9621b844f692943960d7fb1077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b3166a5cd4d493298f52075284f2354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9729758f4b9c4410b8bc95e62cad67cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67f99b3f4e8e4757ad38c91d52e1611d",
       "IPY_MODEL_053400e801a4462795fa9e419ac5cb6e",
       "IPY_MODEL_4524c55cb0434ff4b57fa375b4714221"
      ],
      "layout": "IPY_MODEL_15d0bfbe6dca4ab7b4f9232fca710300"
     }
    },
    "9f07b87485f341879da1bbded82ca170": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5e68b2c41654c0a88f0a33d25c566ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdffe38e72e14589b8e02c05a0f7d460",
      "placeholder": "​",
      "style": "IPY_MODEL_60dfd26adca048c6927cb1c317701ae9",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b14c52d996af42318011f89b94a4770e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b357a26e2974408f95ceff070ecdbbcb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd86c15f1efc4c4c8f3572db97272742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1e32da1e3c41d08001ee2f84fbdd20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b3166a5cd4d493298f52075284f2354",
      "placeholder": "​",
      "style": "IPY_MODEL_88ab1a9621b844f692943960d7fb1077",
      "value": " 4/4 [05:41&lt;00:00, 70.87s/it]"
     }
    },
    "fa2f5d1ddd174838a8816ba1d0643fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdffe38e72e14589b8e02c05a0f7d460": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
